\documentclass{pkuthesis}
\newcommand{\preTitleCn}{}
\newcommand{\preTitleEn}{}
\newcommand{\titleCn}{基于强化学习的法律推理模型}
\newcommand{\titleEn}{Lawyer-Zero-7B}
\newcommand{\studentName}{王昱琛}
\newcommand{\studentID}{2100013153}
\newcommand{\schoolName}{北京大学}
\newcommand{\majorIn}{智能科学与技术}
\newcommand{\tutorName}{冯岩松}
\usepackage[style=gb7714-2015]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage{enumitem} % For customizing lists
\usepackage{graphicx}
% 配置参考文献样式

% 设置参考文献
\defbibheading{bibliography}[\refname]{
    \section*{#1}
    \addcontentsline{toc}{section}{#1}
    \vspace*{16.5pt}
}

\addbibresource{ref.bib}
\begin{document}


% \tableofcontents
\newpage

\section{引言}
\subsection{研究背景}
随着人工智能技术的飞速发展，大语言模型（LLM）在自然语言处理（NLP）领域取得了显著进展，特别是在推理、理解和生成等任务中表现优异。然而，法律领域具有其独特的复杂性与挑战性，主要体现在法律问题的答案往往具有多样性，且法律知识的推理过程较为复杂，这使得在法律领域应用强化学习（RL）技术更加困难。现在的大模型的推理能力，基本是通过数学或者代码进行训练而产生的，但现实生活中，其他领域也会需要强大的推理能力，所以理论上，大语言模型应该也能通过其他领域的复杂问题提升自己的推理能力。本文则是结合使用法律问题与现有的后训练方法以使得模型获得推理能力。

传统的RL方法在处理具有明确答案的任务时表现优异，但在面对法律领域这种存在多种合理解读的情形时，设计合适的奖励模型成为一大难题。因此，如何有效地构建法律推理模型，并通过RL技术优化其推理过程，成为了当前亟待解决的问题。

最近的工作，比如 DeepSeek-R1，Qwen， Seed 系列等模型探索了强化学习算法增强大语言模型的推理能力的方式。这些研究表明，通过仅提供问题和答案，模型可以通过RL算法自发地学习正确的推理方法（Guo等，2025）。DeepSeek-R1（Guo等，2025）和QWQ（Team，2025）说明了如何在数学和编码任务中实现这一目标，而最近的许多努力已经在这些领域中复制了“AHA时刻”（Yeo等，2025; Zeng et al.，2025; Luo等，2025; Luo等，2025）。但是，在其他领域（例如法律）进行有限的尝试。对数学和编码任务的关注是由于基本答案的可用性，可以使用确定性算法轻松验证（Mercer等，2025）。这对于依赖基于结果的奖励的RL算法至关重要。结果的正确性是指导大型学习过程的唯一监督信号。对结果的错误验证可能会导致模型学习错误的政策和训练失败（Weng，2024）。

但是，评估开放式问题的答案的正确性是具有挑战性的，因为许多正确的答案通常强调不同的点或使用不同的分析框架，并具有不同的表达方式，这使设计基于规则和基于模型的奖励模型变得困难，而后者也很容易受到影响（Weng，2024）。肯定要挑战模型是否没有作为人类专家的强大法律推理能力。要在缺乏有效结果奖励模型的法律等领域中应用RL算法，我们通过探索不定项选择题并结合RL训练方法。在RL中使用不定项选择题的主要优点是双重的。首先，不定项选择题（以下简称“多选题”）提供了确定的答案，这大大简化了基于规则的奖励模型的开发。与在不受约束的答案空间内运行的自由形式不同，多选题预先定义了一组有限的候选选项。LLM需要评估这些选项的正确性，从而在有限且可验证的答案空间内运行。这可以使精确的奖励分配并促进实施基于规则的奖励机制。其次，与其他问题类型相比，多选题格式需要更强大的推理和知识获取，例如True/False判断题，该问题也可以使用基于规则的奖励模型进行评估。但是多选题需要对所有可能的选项及其组合进行详尽的评估。该要求确保模型必须仔细检查每个选项并认识到它们之间的细微差异。正确则提供正面奖励，以提供完全准确的答案；任何不正确或丢失的选项都不会导致任何奖励。这种严格的加强机制强制实施了所有可能性的全面考虑，从而与我们旨在通过强化学习灌输的严格推理过程保持一致。

在构建基于推理的多项选择问题的第一步中，我们从中国国家司法审查中选择了10,561个问题。这些问题是基于案例的，每个问题都基于特定的事实描述。收集到的数据集涵盖了中国的主要法律领域，例如民法，刑法和程序法，甚至需要对专业律师进行深入思考和严格的推理。使用基于案例的问题来训练与基于案例的法律教学的核心原则相一致，这种方法要求该模型澄清法律关系，确定适用的法律并在复杂场景中进行全面的推理，从而增强其法律推理的能力。我们认为，这种方法不仅可以提高模型在多选题上的性能，还可以使其能够转移获得的知识和技能以解决更广泛的法律任务。

在RL训练的第二步中，我们使用QWEN2.5-7B-Instruct（Team，2024; Yang等，2024）作为基本模型，并使用GRPO算法进行训练。我们设置了两种类型的奖励：一种是准确性奖励，一种严格的奖励机制，仅当模型选择所有正确的选项时才提供积极的反馈，而否则零奖励。另一个是格式奖励，它要求模型的响应包括 \texttt{<think>} 和 \texttt{<答案>} 标签的结构，以在解决问题时清楚地概述模型的推理过程。在大约两个时期的训练之后，与QWEN2.5-7B实验室相比，由此产生的Deep-Lawyer-Zero-7b模型将准确性从$29.5\%$提高到$57.0\%$，从而验证了训练的有效性。为了探索基于多选题的RL是否可以真正增强模型的法律分析能力，我们使用Lexeval（Li等，2025）来评估法律领域中六种类型的能力，例如理解，逻辑推理和一代。结果表明，与QWEN2.5-7B-Instruct相比，接受RL训练的模型在所有能力方面都取得了显着提高，平均增加了$16.0\%$。此外，此RL方法的表现优于简单的蒸馏方法。例如，使用与DeepSeek R1的答案相同的多项选择问题进行监督的填充（SFT），将QWEN2.5-7B教学的提高仅$2.9\%$，远小于通过我们的多选题 RL训练获得的改进。这表明我们提出的方法是增强模型的法律推理能力的有效且可靠的方法。

进一步的分析表明，在RL训练后，模型的推理过程大大改善。它偏爱利用相关的法律理论，例如犯罪的要素，通过各种镜头剖析问题。它还可以为当前事实找到更多适当的文章。使用不同的观点或逐项分析，它的响应变得更加结构化。此外，该模型的推理路径缩短了正确回答问题的问题，表明更有效地解决问题以实现更简单的任务。相反，对于错误的答案，观察到较长的推理路径，这表明对复杂场景的更多探索。这种现象与在数学领域的观察（Wang等，2025）。

\subsection{研究目标与贡献}
本研究旨在探索后训练方法（包括强化学习和监督微调的结合）在法律领域中的应用，特别是聚焦于通过强化学习优化大语言模型的法律推理能力。我们提出了一种基于不定项多选题（多选题）的强化学习方法，利用多选题的多答案特性设计了规则基础的奖励模型，从而使得模型能够在复杂的法律问题上进行有效推理。

\subsection{论文结构}
本文结构安排如下：
\begin{itemize}
    \item 第二章为相关工作，回顾了强化学习在大语言模型中的应用以及法律推理领域的相关研究。
    \item 第三章详细介绍了本文提出的强化学习方法及其与监督微调的混合训练方法。
    \item 第四章介绍了实验设计和数据集的选择，重点介绍了中国国家司法考试的多选题问题数据集。
    \item 第五章展示了实验结果，并通过与其他方法进行对比，分析了我们方法的优越性。
    \item 第六章对研究进行了讨论，分析了方法的优势与挑战，并提出了未来可能的改进方向。
    \item 第七章为结论，总结了本文的研究成果，并展望了未来的研究方向。
\end{itemize}

\section{相关工作}
近年来，强化学习被广泛应用于大语言模型（LLM）的微调与对齐过程。其中，基于人类反馈的强化学习（RLHF）成为主流技术，用于训练奖励模型以捕获人类偏好，并以此优化语言模型的输出策略。此类方法通常引入额外的奖励模型 $r_{\phi}(x, y)$ 对输出进行评分，通过近端策略优化（PPO）等算法进行策略更新 $\pi_{\theta}(y \mid x)$，使模型行为更符合人类预期。此外，也有研究提出使用人工智能系统提供的反馈（RLAIF）或直接偏好优化（DPO）等替代策略，以减少奖励模型训练的复杂度。这些技术的提出表明，RL工具箱中的多种方法均可用于增强LLM的能力和可控性。近期的多个工作也展现出，强化学习是使得LLM学会推理的重必经之路。

LLM的后训练（post-training）技术旨在超越通用预训练模型，在特定领域或任务上进一步提升性能。从而催生了如OpenAI-o1/o3和DeepSeek-R1等大型后训练模型（LRMs）的发展。具体而言，后训练策略包括利用领域专用数据进行微调、通过RLHF进行人类偏好对齐，以及设计复杂训练流程以增强多步推理能力。这些方法共同构建了LLM从通用能力向领域能力演进的研究框架。

在法律推理领域，研究者已经开始探索LLM的应用潜力，但依然面临严苛要求和挑战。法律应用要求极高的准确性，对于模型推理的中间思考过程有着极高的要求，而目前的LLM在复杂合同和案例分析中尚无法达到这些标准。同时，针对法律领域的评测和综述工作逐渐增多。总体而言，这些研究表明，尽管LLM在法律推理中展现出一定的潜力，但要满足司法应用的高标准仍需不断改进模型能力和可信性。律师美洲驼（Huang et al.，2023）是法律任务的绿色羊石版本，在案例分析和法律推理中证明了绩效的提高。Chatlaw（Cui等，2023）利用法律知识检索，专家的混合和多代理方法来增强法律领域的能力。Lawgpt（Zhou等，2024）在此域特异性数据集中构建了一个法律域数据集以及高质量的法律质量质量质量质量质量标准和中国基本模型。这些作用突出了LLM在法律领域中的潜力，但大部分仅限于监督的微调。我们的工作应用了LLM训练中的强化学习方法，刺激其推理和推广能力。DeepSeek R1成功（DeepSeekai，2025）的大型语言模型增强学习，最近的工作着重于利用强化学习方法来增强大语言模型的推理能力。诸如近端策略优化（PPO）之类的多种方法（Schulman等，2017），（GRPO）（Shao等，2024b），增强++（HU，2025）已应用于后训练，并结合了基于规则的准确性奖励和格式奖励。这种训练后的程序表明在数学问题中有效性（Hu等，2025; Zeng等，2025; Pan等，2025）。Huggingface（2025）利用更复杂的奖励，例如余弦计划的奖励和重复奖励（Yeo等，2025），以进一步提高推理能力并稳定训练后过程。此外，Xie等。（2025）训练该模型的简单逻辑难题，并观察数学问题的显着改善，这表明增强学习可能意味着在范围外的概括中可能有很大的潜力。

在评测法律任务时，多选题形式经常被用于衡量模型能力。例如，中国的LawBench评测基准包含了单标签分类和多标签分类等任务，其中的多标签分类等价于多项选择题。与仅含单一选择题的早期基准不同，LawBench还增加了法律实体识别、阅读理解、咨询等多种任务，以更贴近实际法律应用场景。美国的LegalBench基准则收录了162个法律推理任务，涵盖了广泛的案件分析与法规检索问题。这些基准的设计暗示，仅使用传统多项选择题进行评测可能无法全面衡量模型的法律推理能力，需要结合更复杂的任务类型来评估LLM在法律领域的综合表现。

在系统和工具方面，针对RLHF训练出现了一些创新框架。例如，Sheng等人提出的HybridFlow框架设计了一种混合控制器编程模型，用以高效执行复杂的RLHF数据流。该框架结合了单控制器和多控制器范式的优点，实现了对计算与通信流程的灵活编排，据报告在运行多种RLHF算法时可带来1.5~20.5倍的吞吐量提升。基于HybridFlow理念，由ByteDance Seed团队开发了开源的verl（Volcano Engine RL for LLMs）库。verl提供了易于扩展的API，可快速实现各种RL算法（如PPO、GRPO等），并支持与现有LLM训练基础设施（如FSDP、Megatron-LM、vLLM推理引擎、SGLang等）的无缝集成。此外，verl通过高效的多阶段模型重分片技术（3D-HybridEngine），消除了训练与推理切换时的内存冗余，从而进一步提升了大型模型训练的资源利用效率。

除了模型训练框架，推理阶段的高效引擎也是研究热点。vLLM是一个面向LLM推理的高吞吐量库，采用分页注意力（Paged Attention）等技术来优化GPU内存管理。实验结果表明，与传统的HuggingFace Transformers和OpenAI TGI等框架相比，vLLM在相同硬件资源下可实现数倍以上的吞吐量提升，同时大幅减少KV缓存浪费。vLLM支持多种主流模型架构（如LLaMA、Mistral、Granite、DeepSeek等），并提供量化与工具调用等扩展功能。这些优化使得在法律等领域部署大型LLM时，推理效率和可扩展性得到显著提高。

\section{方法}
在本研究中，我们采用了多种基于强化学习（Reinforcement Learning, RL）的方法对大语言模型（LLM）进行后训练，以提升其在法律推理任务中的表现。特别地，我们重点引入了\textbf{Group Relative Policy Optimization（GRPO）}算法，这是一种对传统近端策略优化（Proximal Policy Optimization, PPO）算法的重要改进，显著减少了对价值函数（Value Function）模型的依赖，同时在数学推理等高阶任务中展现出更优性能。

\subsection{基于人类偏好的强化学习（RLHF）流程}
我们首先采用标准的基于人类反馈的强化学习（RLHF）方法对预训练语言模型进行微调。该流程包括以下几个关键步骤：
\begin{enumerate}
    \item \textbf{奖励模型训练}：使用人工标注的偏好对样本对（prompt $x$, response $y_1$, response $y_2$）进行训练，使奖励模型 $r_{\phi}(x, y)$ 能够根据提示 $x$ 和回答 $y$ 评估回答质量。
    \item \textbf{策略优化}：在奖励模型的指导下，通过策略梯度方法优化语言模型策略 $\pi_{\theta}(y \mid x)$，提高其生成高分回答的能力。
\end{enumerate}
近端策略优化（PPO）（Schulman等，2017）是一种参与者-批评RL算法，在LLMS的RL微调阶段广泛使用（Ouyang等，2022）。然而，传统RLHF流程中的PPO方法需要联合训练一个额外的值函数网络（critic），这不仅增加了计算开销，也可能引入额外的不稳定性。因此，我们进一步引入了GRPO算法以替代PPO。

\textbf{PPO 方法}： 该方法是一种基于策略梯度的算法，通常结合值函数和广义优势估计（GAE）来估计策略的优势函数 $\hat{A}_t$，并在奖励中加入 KL 散度惩罚项以稳定训练过程。PPO的优化目标通常表示为：
$$L^{PPO}(\theta) = \mathbb{E}_{(x,y) \sim D_{\pi_{\text{ref}}}} \left[ r_{\text{model}}(x, y) - \beta \text{KL}(\pi_{\theta}(y \mid x) \| \pi_{\text{ref}}(y \mid x)) \right]$$
其中 $r_{\text{model}}(x, y)$ 是奖励模型对生成结果 $y$ 的评分，$\pi_{\text{ref}}$ 是参考策略（通常为经过监督微调的预训练模型），$\beta$ 是 KL 惩罚系数。
策略更新则采用带有截断（clipping）的代理目标函数：
$$ L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right] $$
其中 $r_t(\theta) = \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}$ 是当前策略与旧策略在时刻 $t$ 采取动作 $a_t$ 的概率比值，$\hat{A}_t$ 是通过 GAE 计算得到的优势估计值。该优化目标结合了概率比值和截断机制，可以在策略更新时限制步长，提升训练稳定性。然而，传统的 PPO 在大规模语言模型中存在两大挑战：一方面，为了计算 GAE，算法需要额外训练一个值函数网络来估计状态的基线价值，这对模型参数规模和训练成本的要求极高；另一方面，在文本生成任务中通常只有序列生成结束时才有一个整体的回报信号（例如最后一个 token 的反馈），这导致对中间状态的价值估计非常困难，从而影响训练过程的稳定性和效率。



\subsection{Group Relative Policy Optimization（GRPO）}
GRPO 是一种无需值函数的策略优化算法，其核心思想是通过模型自身生成的一组回答样本进行相对评分，进而估计优势函数（Advantage Function），从而更新策略。该方法具有如下特点：
\begin{enumerate}
    \item \textbf{无需训练值函数模型}：传统PPO中的优势函数估计依赖于值函数 $V(x)$。而GRPO摒弃了值函数模型，完全基于策略本身采样的相对信息。
    \item \textbf{样本组评分机制}：给定一个输入 $x$，从当前策略 $\pi_{\theta}$ 中生成多个回答 $\{y_1, \dots, y_K\}$，并利用奖励模型对它们打分：
    $$ r_i = r_{\phi}(x, y_i), \quad i = 1, \dots, K $$
    然后按照打分结果对样本进行排序，并为每个样本分配一个排名权重 $\alpha_i$（例如，通过Softmax归一化后的得分），用于构造相对优势。
    \item \textbf{优势函数估计}：
    $$ A(x, y_i) = \alpha_i - \frac{1}{K} \sum_{j=1}^K \alpha_j $$
    其中 $\alpha_i$ 可以为softmax函数输出（例如温度控制的归一化打分），或者为单调映射的排名指标（如逆序数）。
    \item \textbf{策略更新目标}：
    类似PPO，GRPO仍使用一个裁剪（clipping）策略目标来限制策略偏移，但其优势函数来源不同：
    $$ L^{GRPO}(\theta) = \hat{\mathbb{E}}_{(x,y_i) \sim \pi_{\theta_{\text{old}}}} \left[ \min(r_{\theta}(x,y_i)A(x,y_i), \text{clip}(r_{\theta}(x,y_i), 1-\epsilon, 1+\epsilon)A(x,y_i)) \right] $$
    其中：
    $$ r_{\theta}(x, y) = \frac{\pi_{\theta}(y \mid x)}{\pi_{\theta_{\text{old}}}(y \mid x)} $$
\end{enumerate}

\textbf{GRPO 方法}： 该方法不使用显式的值函数网络，而是通过对一个输入提示下生成的多个样本进行相对比较来构造优势信号：具体而言，对于每个提示 $x$，从当前策略中采样生成 $K$ 个不同的回答 $\{y_1, y_2, \dots, y_K\}$，并利用奖励模型对这 $K$ 个回答分别打分 $r_i = r_{\phi}(x, y_i)$。接着对这些得分应用 Softmax 函数（或其他归一化方法），得到每个回答的相对得分 $\alpha_i$，该得分反映了该回答在组内的相对好坏。例如，使用Softmax：
$$ \alpha_i = \frac{\exp(r_i / \tau)}{\sum_{j=1}^K \exp(r_j / \tau)} $$
其中 $\tau$ 是温度参数。然后基于所有回答得分的均值，定义第 $i$ 个回答的相对优势为：
$$ A(x, y_i) = \alpha_i - \frac{1}{K} \sum_{j=1}^K \alpha_j $$
换言之，如果某个回答的评分高于组内均值，则该回答获得正的优势，否则获得负的优势。GRPO 通过这种组内比较直接给出了一个相对评估，无需训练额外的值函数。在策略更新阶段，GRPO 仍采用与 PPO 相似的目标函数格式：是一种强化学习算法，通过在一组采样动作之间使用相对比较来优化策略，而不是依赖于外部批评模型来估计价值函数。我们选择使用GRPO进行基于规则的奖励进行加固学习，因为很难在法律领域建立可靠的奖励模型。带有奖励 $R = \{r_1, r_2, \dots, r_K\}$ 的输出 $O = \{o_1, o_2, \dots, o_K\}$ （此处 $o_i$ 即 $y_i$）的优势被计算为每个组中的归一化奖励：
$$ L^{GRPO}(\theta) = \hat{\mathbb{E}}_{(x, y_i) \sim \pi_{\theta_{\text{old}}}} \left[ \min(r_{\theta}(x,y_i)A(x,y_i), \text{clip}(r_{\theta}(x,y_i), 1-\epsilon, 1+\epsilon)A(x,y_i)) \right] $$
其中 $r_{\theta}(x, y_i) = \frac{\pi_{\theta}(y_i \mid x)}{\pi_{\theta_{\text{old}}}(y_i \mid x)}$。由于不需要值函数网络，GRPO 在内存占用和计算开销上显著减少，更加适用于大模型场景的训练。总体而言，PPO 借助值函数和 GAE 来估计优势并稳定训练，而 GRPO 则通过多样本群体比较直接构造优势信号，后者在模型参数规模庞大且奖励稀疏时能够提高资源利用效率和训练稳定性。

这种设计不仅去除了对值函数的依赖，还充分利用了语言模型自生成内容的结构性差异，从而在训练资源受限的情况下提高样本效率。

\subsection{用于长链推理的 DAPO 算法}
为进一步提升模型在长链推理（Chain-of-Thought, CoT）场景下的能力，我们还采用了近期提出的\textbf{Decoupled Clipped and Dynamic Sampling Policy Optimization（DAPO）}算法。DAPO的关键创新在于：
\begin{itemize}
    \item 解耦裁剪策略与优势函数计算；
    \item 引入动态样本重采样机制，优先保留对训练目标贡献更大的样本；
    \item 结合 verl 框架，在大规模训练中保持训练稳定性和效率。
\end{itemize}
据已有研究报道，DAPO 能显著减少长链推理任务中的训练步骤，提升推理精度与生成质量，尤其适用于逻辑密集型的法律问答任务。

\subsection{奖励函数}
\textbf{格式奖励}。格式奖励的核心目标是实施指导模型推理过程的结构性约束，以确保其响应遵守特定的逻辑框架。具体来说，我们要求模型的输出包括以下结构：\texttt{<think>} 标签和 \texttt{<answer>} 标签，它们在解决问题时明确概述了模型所采取的推理路径（Guo等，2025）。例如，这可能涉及在法律问题中确定有争议的观点，召回相关的法规或判例法，并比较不同的法律概念。

\textbf{准确性奖励}。传统的True/False或多项选择问题在2或4个选项中有一个正确的答案。通常可以通过不完整或表面的推理来解决它们，因为有限的答案空间使模型可以在不完全理解基本概念的情况下获得正确的答案。这种局限性使此类格式在训练强大的推理能力方面的有效性降低了，尤其是在法律等复杂领域。相比之下，具有一个或多个正确答案的多选题问题要求该模型可以全面评估每个选项的正确性，而没有猜测或部分推理的空间。
我们设计了一个严格的奖励机制，仅当模型在所有其他情况下都可以选择所有正确的选项并分配零奖励（奖励$=0$）时，才能提供积极的反馈（奖励$=1$）。令 $S_{\text{true}}$ 为多选题问题的正确选项集合，$S_{\text{pred}}$ 为模型预测的选项集合。则准确性奖励 $R_{\text{accuracy}}$ 定义为：
$$ R_{\text{accuracy}}(S_{\text{pred}}, S_{\text{true}}) = \begin{cases} 1 & \text{if } S_{\text{pred}} = S_{\text{true}} \\ 0 & \text{otherwise} \end{cases} $$
它确保正确的选项与准确和完整的推理过程相关联。这种严格的奖励机制消除了在强化学习过程中基于捷径的解决方案的可能性，并迫使该模型对当前的任务有了更深入的了解。

\section{实验}
\subsection{训练数据集}
我们使用JEC-QA数据集（Zhong等，2020）作为训练数据。JEC-QA是来自中国国家司法审查的法律领域中的大规模提问数据集。该数据集包含一个或多个正确答案的26,365个多项选择问题。JECQA中的问题包含两种类型的问题，知识驱动的问题和案例分析问题。我们选择训练的案例分析问题，因为它们专注于深度法律分析和解决问题的技能，而不是死记硬背的记忆（Patterson，1951）。解决此类问题需要该模型澄清法律关系，确定适用的法律并在复杂场景中进行全面的推理，从而增强其法律推理能力。我们将10,561个分析问题分为训练和测试集的8：2比率，用于训练和域中验证。

\subsection{Baselines}
\begin{itemize}
    \item \textbf{Qwen2.5-7b-教学}：中国领域最强大的LLM之一（Team，2024; Yang等，2024）。我们评估法律领域的零射击性能作为参考。
    \item \textbf{R1蒸馏}：基于QWEN2.5-7B教学的训练数据中的R1回答对R1的回答。提炼强大的模型可以显着提高数学和编码中较小模型的性能（Guo等，2025）。该模型是蒸馏方法的基线。
\end{itemize}

\subsection{VeRL}
\textbf{General Concepts}\\
\textbf{RL}: 强化学习（Reinforcement Learning，RL）在近年来因 \texttt{o1/r1} 等技术突破而成为大模型训练的热门方向。VeRL 是一款面向 RL 的高效训练框架。从自然语言处理（NLP）的视角来看，RL 与传统的监督微调（SFT）主要有以下差异：
\begin{enumerate}
    \item \textbf{引入惩罚信号}：SFT 仅模仿正例，而 RL 同时对优质样本给予奖励、对劣质样本施加惩罚。无论策略梯度、GRPO、Reinforce 还是 PPO，本质上都在设计奖励／惩罚的颗粒度（token / macro action / sequence 等）与强度（是否使用 baseline、KL 约束、clip 等）。
    \item \textbf{允许模型自采样并在线训练自身}：SFT 通常依赖人工标注或其他模型生成的数据（蒸馏）。RL 则可实时采样并利用当前策略更新模型。
\end{enumerate}

\paragraph{Rejection Sampling 与 RL 的关系}
Rejection Sampling（RS）虽也“自采样训练自身”，但默认无惩罚信号（可通过引入负例并进一步采用 DPO）。因此从机制看，RS 更接近 RL，而非 SFT。

\paragraph{on‐policy vs.\ online}
\begin{itemize}
    \item \textbf{online}: 当前策略能否与环境交互并实时获取奖励信号（如数学题求解后立即得知正确与否）。在 GUI Agent、自动驾驶等场景需构建复杂模拟器。
    \item \textbf{on‐policy}: 训练数据是否由最新策略采样。在实践中常预采大量经验数据再分 mini‐batch 更新，除首个 mini‐batch 外均为 off‐policy。
\end{itemize}
常用的 GRPO/Reinforce/PPO 等方法一定是 \emph{online}，但不必然 \emph{on‐policy}（取决于 mini‐batch 数）。

\paragraph{Ray 系统概览}
Ray 是一套分布式计算框架，为 VeRL 和 OpenRLHF 等 RL 框架提供 Actor 角色管理及资源调度。核心概念如下：
\begin{itemize}
    \item \textbf{Ray Actor}: 有状态远程任务（由 \texttt{ray.remote} 装饰的 Python 类），运行时对应独立进程（勿与 RL 中的“Actor”角色混淆）。
    \item \textbf{Ray Task}: 无状态远程任务（由 \texttt{ray.remote} 装饰的函数），局部变量对提交方不可见，可视作无状态。
    \item \textbf{资源管理}: Ray 可按 CPU/GPU/内存等进行自动调度，也支持 \textit{placement group} 将 Actor 固定在同一或不同设备 bundle 上。
    \item \textbf{异步执行}: Ray 调度默认异步，任务提交即返回对象引用；用户可用 \texttt{ray.get}／\texttt{ray.wait} 阻塞或轮询结果。
\end{itemize}
在 RL 训练中引入异步设计，可让 Actor／Critic／Generator／RM 等角色的计算流水重叠，例如在 Actor 更新上一批数据时，Generator 已可并行生成下一批样本。鉴于 o1‐style RL 的主要瓶颈位于 rollout，未来的优化方向是更充分地异步化 rollout（如夜间充分利用线上推理集群空闲算力）。

\paragraph{并行策略}
\begin{itemize}
    \item \textbf{3D 并行}: LLM 训练（megatron‐lm）与推理引擎（vllm、sglang）已广泛支持数据并行（DP）、张量并行（TP）与流水线并行（PP）。VeRL 新版本基于 Ulysses 进一步支持序列并行（SP），对长文本 RL 尤为关键。
    \item 不同角色在不同阶段可灵活调整 3D 并行组合，VeRL 借助 \emph{hybrid engine} 做了诸多优化，如零冗余参数 re‐sharding。
\end{itemize}

\paragraph{FSDP 与 Megatron}
FSDP（Meta 提出）与 Megatron 分别代表两套分布式训练框架：
\begin{itemize}
    \item \textbf{FSDP}: 将模型参数（权重、优化器状态等）在 GPU 间分片存储，仅按需通信，并重叠计算与通信，逻辑清晰、易支持新结构，研究友好。
    \item \textbf{Megatron}: 在百亿级模型训练中更具性能优势，参数 re‐sharding 开销低，工程友好。
\end{itemize}
VeRL 同时兼容两套引擎。

\subsubsection{VeRL‐related Concepts}

\paragraph{Hybrid Flow}
RL 训练逻辑涉及多模型交互。VeRL 将数据流抽象为两层：
\begin{itemize}
    \item \textbf{控制流}: 高层描述角色间交互，如 Actor 生成经验后，Critic／RM／Reference 计算得分，然后计算 GAE 与损失。
    \item \textbf{计算流}: 低层描述单角色内的前向‐反向、优化器更新、自回归生成等。
\end{itemize}

\paragraph{Single Controller vs.\ Multiple Controllers}
\begin{itemize}
    \item \textbf{Single Controller}: 单中心控制器统一管理所有子模块，架构清晰、易维护。VeRL 采用该模式实现 RL 算法控制流，极大便利新算法开发。
    \item \textbf{Multiple Controllers}: 将控制逻辑分散到多控制器，通过集合通信同步，通信开销低但逻辑更复杂。VeRL 在\emph{计算流}维度使用此模式，以降低通信负载。
\end{itemize}
VeRL 通过多层级 Worker（\texttt{RayWorkerGroup} $\rightarrow$ \texttt{WorkerDict} $\rightarrow$ \texttt{ModelWorker} $\rightarrow$ \texttt{ParallelWorker}）封装计算流。

\paragraph{Hybrid Engine — 模型放置策略}
\begin{enumerate}
    \item \textbf{分开放置}: 角色独立占用设备，可异步执行但 GPU 利用率略低。
    \item \textbf{分组放置}: 将角色分组共置，既能重叠执行又减少空闲：
    \begin{itemize}
        \item 典型分组：Actor/Generator 同组（需实时同步参数），Critic/RM 与 Reference 分别单独。
    \end{itemize}
    \item \textbf{全部共置}: 所有角色共用设备，GPU 始终被占用，但只能串行。
\end{enumerate}
VeRL 通过 \texttt{resource pool} 灵活支持以上策略，并设计 \texttt{worker\_dict} 以动态角色切换（\texttt{reload/offload params}）。

\paragraph{数据传输协议}
为适配不同角色的数据切分需求，VeRL 设计了统一的数据分发（Dispatch）与收集（Collect）协议，并以 Python 装饰器形式绑定到 Worker 方法，实现透明的数据流与执行模式。

\paragraph{训练流程示意}
\begin{enumerate}
    \item \texttt{RayPPOTrainer} 向 \texttt{RayWorkerGroup} 发送方法调用。
    \item \texttt{RayWorkerGroup} 先执行数据分发，再根据执行模式决定哪些 Worker 执行任务。
    \item 结果经收集逻辑处理后返回 Trainer。
\end{enumerate}

\subsubsection{RL 设置}
本研究的 RL 阶段采用 VeRL（Sheng et al., 2024）实现组相对策略优化（GRPO）。以下结合 VeRL 的特性，阐述 GRPO 实现步骤。

\paragraph{1.\ 初始化与核心组件}
GRPO 训练由 \texttt{GRPOTrainer} 管理，初始化需配置：
\begin{itemize}
    \item \textbf{策略模型（Actor）}: 采用 QWEN2.5‐7B‐Instruct。
    \item \textbf{参照模型（Reference）}: 同架构 SFT 模型，用于计算 KL 惩罚。
    \item \textbf{奖励函数}: 采用基于规则的格式与准确性奖励，无需外部 RM。
\end{itemize}

\paragraph{2.\ Rollout 生成}
\begin{itemize}
    \item \textbf{权重同步}: 每轮大迭代开始，通过 \texttt{RolloutShardingManager} 将最新 Actor 权重同步到推理引擎（vLLM／SGLang）。
    \item \textbf{提示采样}: 从数据集中抽取一批 prompt，由 \texttt{generate\_sequences} 调用推理引擎生成响应。
    \item \textbf{多样生成}: 配置 \texttt{actor\_rollout\_ref.rollout.n}（本研究设 $K{=}7$）并设置 \texttt{temperature=1.0}，为每个 prompt 并行生成 $K$ 条不同响应。
    \item \textbf{奖励计算}: 对每条响应计算格式与准确性奖励 $r_i$（详见 \ref{subsection:reward_function}）。
    \item \textbf{数据封装}: 保存 prompt、$K$ 条响应、token 级对数概率、mask 与奖励，组成后续训练批次；prompt 最长 512 token，响应最长 2048 token。
\end{itemize}

\paragraph{3.\ 对数概率计算}
\begin{itemize}
    \item \textbf{旧策略对数概率 $\text{old\_log\_prob}$}: 使用未更新的 Actor 对 (prompt, response) 再次前向传播获取。原因：\textit{(i)} 高性能推理引擎通常不保存完整 token‐level log‐prob；\textit{(ii)} 直接保存可能受数值波动与并行策略影响。
    \item \textbf{参照策略对数概率 $\text{ref\_log\_prob}$}: Reference Model 对同一批数据计算 log‐prob，用于 KL 约束。首轮迭代时与 \text{old\_log\_prob} 相同。
\end{itemize}

\paragraph{4.\ 奖励与优势估计}
\begin{itemize}
    \item \textbf{奖励整合}: VeRL 支持外部 RM 或自定义函数（本研究采用规则奖励）。
    \item \textbf{优势函数}: 调用 \texttt{compute\_advantage}，依据 GRPO 公式（公式 \textit{Y}）在组内（同一 prompt 的 $K$ 样本）计算相对优势，并可选择标准化 (\texttt{norm\_adv\_by\_std\_in\_grpo})。
\end{itemize}

\paragraph{5.\ 策略更新（Mini‐batch 内循环）}
对每个 mini‐batch 执行：
\begin{itemize}
    \item 计算新策略对数概率 $\text{new\_log\_prob}$。
    \item \textbf{策略梯度损失} $\text{pg\_loss}$：由 \texttt{compute\_policy\_loss} 依据公式 \textit{Z} 计算，含 cliprange 等裁剪。
    \item \textbf{熵正则}：鼓励探索。\texttt{entropy\_coeff} 控制权重。
    \item \textbf{KL 惩罚}：当前策略与 Reference 之间的 KL 散度，乘 \texttt{kl\_loss\_coef} 加入总损失。
    \item 反向传播并更新 Actor 参数。
\end{itemize}
完成全部 mini‐batch 后，同步最新权重，进入下一大迭代的经验收集。

\subsubsection{蒸馏设置}
我们利用LlamaFactory（Zheng等，2024）将推理模式从DeepSeek-R1提炼出来。在实验过程中，我们使用R1的所有响应（包括答案不正确的响应）训练模型。我们观察到，与仅使用正确的响应相比，这种设置会导致卓越的性能。该模型接受了2个时期的训练，其批量大小为32，上下文长度为4096。我们采用了 $1 \times 10^{-5}$ 的学习率，结合了余弦学习率调度程序和 $10\%$ 的热量。

\subsection{Metric评测}
由于每个多选题示例都有一个或多个答案，因此我们评估了实例和选项级别的性能。给定的答案 $y^{(i)}_{\text{pred}}$ 和地面真相 $y^{(i)}_{\text{gold}}$ 对于第 $i$ 个问题，两个指标表示如下：

\textbf{精确匹配（实例级）}：这衡量了预测答案与正确答案完全匹配的实例的比例。
$$ \text{精确匹配} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}(y^{(i)}_{\text{pred}} = y^{(i)}_{\text{gold}}) $$
其中 $\mathbf{1}(\cdot)$ 是指示函数，如果内部的条件为true，则等于1，否则为0。它评估模型是否可以为问题提供完全正确的答案。对于多选题，这意味着预测的选项集合与真实的选项集合完全一致。

\textbf{召回和精确度（选择级）}：我们根据模型对单个答案选项的预测来计算召回和精度得分。
令 $N$ 为问题总数。对于第 $i$ 个问题，令 $P_i$ 为模型预测的正确选项集合，$G_i$ 为实际的正确选项集合。
则选项级别的精确度 (Precision) 和召回率 (Recall) 定义为：
$$ \text{Precision}_{\text{option}} = \frac{\sum_{i=1}^N |P_i \cap G_i|}{\sum_{i=1}^N |P_i|} $$
$$ \text{Recall}_{\text{option}} = \frac{\sum_{i=1}^N |P_i \cap G_i|}{\sum_{i=1}^N |G_i|} $$
其中 $|S|$ 表示集合 $S$ 的基数。如果某个问题模型没有预测任何选项（即 $|P_i|=0$），则该问题对精确度的分子和分母的贡献均为0。
通常也会计算 F1 分数：
$$ F1_{\text{option}} = 2 \cdot \frac{\text{Precision}_{\text{option}} \cdot \text{Recall}_{\text{option}}}{\text{Precision}_{\text{option}} + \text{Recall}_{\text{option}}} $$

\section{实验思路与尝试}
\begin{enumerate}[label=\arabic*.]
    \item \textbf{对于reward function的构建和尝试}
    因为Simple-R1中提到，模型在GRPO过程中会迅速学习到format reward但是accuracy reward需要更长时间，他们认为去除format reward可以给模型更大的自由探索的空间，所以我尝试了不给format reward的情形，此时在测试集上只有0.564333017975402。有小幅的下降。
    \begin{itemize}
        \item 最开始使用的是完全匹配，只有回答和正确答案完全一致时，得分为1，否则为0。即：
        $$ R_{\text{exact}}(S_{\text{pred}}, S_{\text{true}}) = \begin{cases} 1 & \text{if } S_{\text{pred}} = S_{\text{true}} \\ 0 & \text{otherwise} \end{cases} $$
        \item 之后我尝试了对于选错、漏选情况适量给分，完全一致时为1，在此基础上每选择一个错误选项或者漏选一个正确选项则扣 $x$ 分，考虑到希望模型不要学习到short cut有意的少选或者多选，$x$ 应该大于等于0.5。
        令 $FP = |S_{\text{pred}} \setminus S_{\text{true}}|$ (错误选择的选项数) 且 $FN = |S_{\text{true}} \setminus S_{\text{pred}}|$ (漏选的正确选项数)。
        则部分奖励 $R_{\text{partial}}$ 可以定义为：
        $$ R_{\text{partial}}(S_{\text{pred}}, S_{\text{true}}) = \max(0, 1 - x \cdot (FP + FN)) $$
        其中 $x \ge 0.5$。
        当x为0.9的时候，测试集的accuracy为0.5638599810785241    
    \end{itemize}
    \item \textbf{蒸馏}
    我首先尝试的是采用DeepSeek-R1-Distill-Qwen-7B进行测试，我希望通过蒸馏后的qwen能学习到long COT的推理路径形成正确的推理以及探索。
    与通过DeepSeek-R1生成的法考多选题的COT进行对比，希望得到相同的效果。
    \item \textbf{对于强化学习方法的各种尝试}
尝试了GRPO Dr GRPO VAPO DAPO 等方法
    \item \textbf{对于数据的修改和清洗}
    对于本来就能做对以及怎么都做不对的题进行删除（rollout为6的情况下）。
    \item \textbf{收集合适的SFT训练数据并且进行SFT}
    \item \textbf{其他实验方向}
    \begin{enumerate}[label=\arabic*.]

        \item 0.9 0.
        \item no format (不使用格式奖励的实验)
        \item Dr grpo (可能是 DeepSpeed-Chat GRPO 或其他变体)
        \item DAPO
        \item VAPO (Value-Augmented Policy Optimization 或其他)
        \item 清洗排序数据
        \item 蒸馏 + RL
        \item zero + RL (zero-shot prompting + RL)
        \item length 方面 (对生成长度的控制或分析)
        \item maybe
        \begin{itemize}
            \item Second RL (第二阶段RL)
            \item LCPO (Likelihood Clipped Policy Optimization 或其他)
        \end{itemize}
    \end{enumerate}
\end{enumerate}

\printbibliography
\end{document}